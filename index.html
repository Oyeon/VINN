<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <!-- Social share meta tags -->
  <meta property="og:title" content="The Surprising Effectiveness of Representation Learning for Visual Imitation">
  <meta property="og:description" content="A new way for visual imitation from demonstrations: Visual Imitation Through Nearest Neighbors, or VINN">
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Visual Imitation Through Nearest Neighbors">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="The Surprising Effectiveness of Representation Learning for Visual Imitation">
  <meta name="twitter:description" content="A new way for visual imitation from demonstrations: Visual Imitation Through Nearest Neighbors, or VINN">
  <meta name="twitter:image" content="https://https://jyopari.github.io/VINN/files/method.png" />
  <!-- End social share meta tags -->
  <link rel="shortcut icon" href="img/favicon.png">
  <link rel="stylesheet" href="css/simple-grid.css">
  <title>The Surprising Effectiveness of Representation Learning for Visual Imitation</title>
  <meta name="description" content="A simple, lightweight grid and container system for your website.">
  <!-- End Google Analytics -->
</head>
<body>
  <div class="jumbotron">
    <div class="container">
      <div class="row">
        <div class="col-12 center">
          <h1>The Surprising Effectiveness of Representation Learning for Visual Imitation</h1>
        </div>
      </div>
      <div class="row">
        <div class="col-3 hidden-sm"></div>
        <div class="col-2 center">
          <a href="https://arxiv.org/abs/2112.01511" download>
            <h3>Paper</h3>
          </a>
        </div>
        <div class="col-2 center">
            <a href="https://github.com/jyopari/VINN/tree/main" download>
              <h3>Code</h3>
            </a>
          </div>
          <div class="col-2 center">
            <a href="https://drive.google.com/drive/folders/11-sAN2a-F7G-lvx6qRXnrWjlxNb0PH1m" download>
              <h3>Data</h3>
            </a>
          </div>

        <div class="col-3 hidden-sm"></div>
      </div>
      <div class="row">
        <div class="col-3 center">
          <h3>Jyothish Pari*</h3>
          <p>
            New York University
          </p>
        </div>
        <div class="col-3 center">
            <h3>Nur Muhammad (Mahi) Shafiullah*</h3>
            <p>
              New York University
            </p>
          </div>
          <div class="col-3 center">
            <h3>Sridhar Pandian Arunachalam</h3>
            <p>
              New York University
            </p>
          </div>
          <div class="col-3 center">
            <h3>Lerrel Pinto</h3>
            <p>
              New York University
            </p>
          </div>
      </div>
      <div class="row">
        <div class="col-1 hidden-sm"></div>
        <div class="col-10 center img">
          <div class="auto-resizable-iframe">
            <div>
              <iframe src="https://www.youtube.com/embed/bIzWADZjdoA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
            </div>
          </div>          
          
        </div>
        <div class="col-1 hidden-sm"></div>
      </div>
      <div class="row">
        <div class="col-12">
          <h2 class="center m-bottom">Abstract</h3>
          <p>
            While visual imitation learning offers one of the most effective ways of learning from visual demonstrations, generalizing from them requires either hundreds of diverse demonstrations, task specific priors, or large, hard-to-train parametric models.
One reason such complexities arise is because standard visual imitation frameworks try to solve two coupled problems at once: learning a succinct but good representation from the diverse visual data, while simultaneously learning to associate the demonstrated actions with such representations.
Such joint learning causes an interdependence between these two problems, which often results in needing large amounts of demonstrations for learning.
To address this challenge, we instead propose to decouple representation learning from behavior learning for visual imitation. First, we learn a visual representation encoder from offline data using standard supervised and self-supervised learning methods. Once the representations are trained, we use non-parametric Locally Weighted Regression to predict the actions.
We experimentally show that this simple decoupling improves the performance of visual imitation models on both offline demonstration datasets and real-robot door opening compared to prior work in visual imitation.
          </p>
        </div>
      </div>
      <div class="row">
        <div class="col-12">
          <h2 class="center">Method</h3>
          <img src="files/method.png"/>
          <p>Overview of our VINN algorithm. During training, we use offline visual data to train a BYOL-style self-supervised model as our encoder. During evaluation, we compare the encoded input against the encodings of our demonstration frames to find the nearest examples to our query. Then, our model's predicted action is just a weighted average of the associated actions from the nearest images.</p>
          </div>
        </div>
      </div>
    </div>
  </div>
  <div class="body-content">
    <div class="container">
        <div class="row">
            <div class="col-4 hidden-sm"></div>
            <div class="col-4">
              <div class="line"></div>
            </div>
            <div class="col-4 hidden-sm"></div>
          </div>
        <div class="row">
            <div class="col-1 hidden-sm"></div>
            <div class="col-10">
              <h2 class="center m-bottom">Demonstrations</h3>
            </div>
            <div class="col-1 hidden-sm"></div>
          </div>
      <div class="grid-display">
        <div class="row">
          <div class="col-6">
            <video class="img" controls muted autoplay loop>
                <source src="files/demo_small.mp4" type="video/mp4">
            </video>
          </div>
          <div class="col-6">
            <video class="img" controls muted autoplay loop>
                <source src="files/demo_pov_small.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="row">
            <div class="col-1 hidden-sm"></div>
            <div class="col-10">
              <h2 class="center">Execution</h3>
            </div>
            <div class="col-1 hidden-sm"></div>
          </div>
        <div class="row">
          <div class="col-6">
            <video class="img" controls muted autoplay loop>
                <source src="files/vinn_1_small.mp4" type="video/mp4">
            </video>
          </div>
          <div class="col-6">
            <video class="img" controls muted autoplay loop>
                <source src="files/vinn_2_small.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      <div class="row">
        <div class="col-4 hidden-sm"></div>
        <div class="col-4">
          <div class="line"></div>
        </div>
        <div class="col-4 hidden-sm"></div>
      </div>
      <div class="row">
        <div class="col-1 hidden-sm"></div>
        <div class="col-10">
          <h2 class="center">Citation</h3>
          <pre class="code">
@misc{pari2021surprising,
    title={The Surprising Effectiveness of Representation Learning for Visual Imitation}, 
    author={Jyothish Pari and Nur Muhammad Shafiullah and Sridhar Pandian Arunachalam and Lerrel Pinto},
    year={2021},
    eprint={2112.01511},
    archivePrefix={arXiv},
    primaryClass={cs.RO}
}
          </pre>
        </div>
        <div class="col-1 hidden-sm"></div>
      </div>
    </div>
  </div>
  <footer>
  </footer>
</body>
</html>
